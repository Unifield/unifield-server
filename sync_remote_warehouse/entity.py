from osv import osv, fields, orm
import base64
from zipfile import ZipFile
from cStringIO import StringIO
import csv
import copy
from tools.translate import _
from sync_client import sync_process
import os
from datetime import datetime
import uuid

class Entity(osv.osv):
    _inherit = 'sync.client.entity'

    _columns = {
        # used to determine which sync rules to use 
        'usb_instance_type': fields.selection((('',''),('central_platform','Central Platform'),('remote_warehouse','Remote Warehouse')), string='USB Instance Type'),
        
        # used to ignore all data older than this date when syncing as it is already in the db
        'clone_date': fields.datetime('Backup Date And Time', help='The date that the Central Platform database was backed up to provide the seed data for the Remote Warehouse'),
        
        # the step of the synchronisation process - first_sync, pull_performed, push_performed
        'usb_sync_step': fields.selection((('first_sync','First Synchronisation'), ('pull_performed','Pull Performed'), ('push_performed', 'Push Performed')), 'USB Synchronisation Step'), 
        
        # used to make sure user does not try to import old data
        'usb_last_push_date': fields.datetime('Last Push Date', help='The date and time of the last Push'),
        'usb_last_push_file': fields.binary('Last Push File', help='The zip file that was generated by the last Push'),
    }
    
    _defaults = {
        'usb_sync_step': 'first_sync',
        'usb_instance_type': '',
    }
    
    IMPORT_ERROR_NOT_CSV = 'Not a CSV file'
    IMPORT_ERROR_NOT_OBJECT = 'Could not find object in object pool'
    
    def _usb_update_sync_step(self, cr, uid, step):
        entity = self.get_entity(cr, uid)
        return self.write(cr, uid, entity.id, {'usb_sync_step': step})
    
    @sync_process(step='msg_push', need_connection=False, defaults_logger={'usb':True})
    def usb_create_zip(self, cr, uid, context=None):
        """
        Create packages out of all total_updates marked as "to send", format as CSV, zip and attach to entity record 
        """
        
        update_csv_contents = []
        message_csv_contents = []
        context = context or {}
        total_updates = total_deletions = total_messages = 0 
        entity = self.get_entity(cr, uid)
        
        logger = context.get('logger', None)
        if logger:
            logger_index = logger.append()
        
        def update_create_package():
            return self.pool.get('sync_remote_warehouse.update_to_send').create_package(cr, uid, entity.session_id)

        def update_add_and_mark_as_sent(ids, packet):
            """
            add the package contents to the update_csv_contents dictionary and mark the package as sent
            @return: (number of total_updates, number of delete_sdref rules)
            """
            
            # create header row if needed
            columns = ['source', 'model', 'version', 'fields', 'values', 'sdref', 'is_deleted']
            if not update_csv_contents:
                update_csv_contents.append(columns)
            
            # insert update data
            for update in packet['load']:
                update_csv_contents.append([
                    entity.name,
                    packet['model'], # model
                    update['version'],
                    packet['fields'],
                    update['values'],
                    update['sdref'],
                    False,
                ])
                
            # insert delete data
            for delete_sdref in packet['unload']:
                update_csv_contents.append([
                    entity.name,
                    packet['model'],
                    '',
                    '',
                    '',
                    delete_sdref,
                    True,
                ])
            
            # mark updates_to_send as sent and return number of records processed
            self.pool.get('sync_remote_warehouse.update_to_send').write(cr, uid, ids, {'sent' : True}, context=context)
            return (len(packet['load']), len(packet['unload']))
        
        def message_add_and_mark_as_sent(packet):
            columns = ['id','call','dest','args']
            
            if not message_csv_contents:
                message_csv_contents.append(columns)
            
            # insert message data
            for message in packet:
                message_csv_contents.append([
                    message['id'], 
                    message['call'],
                    message['dest'],
                    message['args'],
                ])
                
            # mark updates_to_send as sent and return number of records processed
            messages_pool.packet_sent(cr, uid, packet, context=context) 
            return len(packet)
        
        _update_rules_serialization_mapping = {
            'id' : 'server_id',
            'name' : 'name',
            'owner_field' : 'owner_field',
            'model' : 'model',
            'domain' : 'domain',
            'sequence_number' : 'sequence_number',
            'included_fields' : 'included_fields',
            'can_delete' : 'can_delete',
            'usb' : 'usb',
            'active': 'active',
            'direction_usb' : 'direction_usb',
        }
    
        def _serialize_update_rule(update_rule_pool, cr, uid, ids, context=None):
            rules_data = []
            for rule in update_rule_pool.browse(cr, uid, ids, context=context):
                rules_data.append(dict(
                    (data, rule[column]) for column, data
                        in _update_rules_serialization_mapping.items()
                ))
            return rules_data
        
        _message_rules_serialization_mapping = {
            'server_id': 'server_id',      
            'name': 'name' ,
            'model_name': 'model',
            'domain': 'domain',
            'sequence_number': 'sequence_number',
            'remote_call': 'remote_call',
            'arguments': 'arguments',
            'destination_name': 'destination_name',
            'active': 'active',
            'usb': 'usb'
        }
        
        def _serialize_message_rule(self, cr, uid, ids, context=None):
            rules_data = []
            for rule in self.browse(cr, uid, ids, context=context):
                rules_data.append(dict(
                    (data, rule[column]) for column, data
                        in _message_rules_serialization_mapping.items()
                ))
            return rules_data

        updates_todo = len(self.pool.get('sync_remote_warehouse.update_to_send').search(cr, uid, [('sent','=',False)], context=context))
        messages_todo = len(self.pool.get('sync_remote_warehouse.message_to_send').search(cr, uid, [('sent','=',False)], context=context))
        
        if not updates_todo and not messages_todo:
            return 0, 0, 0
        
        ################################################### 
        ################# create updates ##################
        ###################################################

        if updates_todo:        
            if logger:
                logger.replace(logger_index, _('Updates to package: %d' % updates_todo))
            
            package = update_create_package()
            
            # add the package to the update_csv_contents dictionary and mark it has 'sent'
            total_updates, total_deletions = update_add_and_mark_as_sent(*package)
                
            # finished all packages so update logger
            if logger and (total_updates or total_deletions):
                logger.replace(logger_index, _("Update packaging complete: %d updates and %d deletions = total of %d") % (total_updates, total_deletions, (total_updates + total_deletions)))
        else:
            if logger:
                logger.replace(logger_index, _('No updates to package'))
        
        ################################################### 
        ################# create messages #################
        ###################################################
        
        if logger:
            logger_index = logger.append()

        if messages_todo:
            if logger:
                logger.replace(logger_index, _('Message(s) to package: %d' % messages_todo))
            
            # prepare packets
            messages_pool = self.pool.get('sync_remote_warehouse.message_to_send')
            packet = messages_pool.get_message_packet(cr, uid, context=context)
            total_messages = len(packet)
            
            # add packet to csv data list
            message_add_and_mark_as_sent(packet)
            if logger:
                logger.replace(logger_index, _("Message(s) packaged: %d") % (total_messages))
        else:
            if logger:
                logger.replace(logger_index, _('No messages to package'))
        
        ################################################### 
        ################# update rules ####################
        ###################################################
                
        # create update rules file to send to remote warehouse if instance is central platform
        if entity.usb_instance_type == 'central_platform': 
            update_rule_pool = self.pool.get('sync.client.rule')
            update_rule_ids = update_rule_pool.search(cr, uid, [('usb','=',True),'|',('direction_usb','=','rw_to_cp'),('direction_usb','=','bidirectional')])
            update_rules_serialized = _serialize_update_rule(update_rule_pool, cr, uid, update_rule_ids, context=context)
            
            update_rules_string_io = StringIO()
            update_rules_string_io.write(str(update_rules_serialized))
                
        ################################################### 
        ################# message rules ###################
        ###################################################
        
        # create message update rules file to send to remote warehouse if instance is central platform
        if entity.usb_instance_type == 'central_platform': 
            message_rule_pool = self.pool.get('sync.client.message_rule')
            message_rule_ids = message_rule_pool.search(cr, uid, [('usb','=',True)])
            message_rules_serialized = _serialize_message_rule(message_rule_pool, cr, uid, message_rule_ids, context=context)
            
            message_rules_string_io = StringIO()
            message_rules_string_io.write(str(message_rules_serialized))
        
        ################################################### 
        ################# create zip ######################
        ###################################################
        
        try:
            # create update csv file
            update_csv_string_io = StringIO()
            update_csv_writer = csv.writer(update_csv_string_io, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            for data_row in update_csv_contents:
                update_csv_writer.writerow(data_row)
                
            # create message csv file
            message_csv_string_io = StringIO()
            message_csv_writer = csv.writer(message_csv_string_io, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            for data_row in message_csv_contents:
                message_csv_writer.writerow(data_row)
                    
            # compress update and message csv file into zip
            zip_file_string_io = StringIO()
            zip_base64_output = StringIO()
            zip_file = ZipFile(zip_file_string_io, 'w')
            
            zip_file.writestr('sync_remote_warehouse.update_received.csv', update_csv_string_io.getvalue())
            zip_file.writestr('sync_remote_warehouse.message_received.csv', message_csv_string_io.getvalue())
            
            # compress update and message rules into zip file
            if entity.usb_instance_type == 'central_platform': 
                zip_file.writestr('update_rules.txt', update_rules_string_io.getvalue())
                zip_file.writestr('message_rules.txt', message_rules_string_io.getvalue())
                
            zip_file.close()
                    
            # add to entity object
            zip_file_contents = zip_file_string_io.getvalue()
            zip_base64 = base64.encodestring(zip_file_contents)
    
            # clean up
            zip_base64_output.close()
            update_csv_string_io.close()
            message_csv_string_io.close()
            if entity.usb_instance_type == 'central_platform':
                update_rules_string_io.close()
                message_rules_string_io.close()
            zip_file_string_io.close()
    
            # attach file to entity and delete 
            self.write(cr, uid, entity.id, {'usb_last_push_file': zip_base64, 'usb_last_push_date': datetime.now()})
            
            if logger:
                logger.switch('status', 'ok')
        except Exception, e:
            if logger:
                logger.append(_('Error while creating zip file: %s' % str(e)))
                logger.switch('data_push', 'failed')
            raise
        finally:
            if logger:
                logger.write()
        
        return (total_updates, total_deletions, total_messages)
    
    @sync_process(step='data_push', need_connection=False, defaults_logger={'usb':True})
    def usb_create_update(self, cr, uid, session, context=None):
        """
        Create update_to_send for a USB synchronization and return a browse of all to-send updates
        """
        
        context = context or {}
        context.update({
            'update_to_send_model': 'sync_remote_warehouse.update_to_send', 
            'last_sync_date_field': 'usb_sync_date'
        })
        
        entity = self.get_entity(cr, uid, context)

        logger = context.get('logger', None)
        if logger:
            logger_index = logger.append()
        update_pool = self.pool.get('sync_remote_warehouse.update_to_send')
        
        def create_update(session):
            updates_count = 0
            
            rule_search_domain = [('usb','=',True)]
            if entity.usb_instance_type == 'central_platform':
                rule_search_domain = rule_search_domain + ['|',('direction_usb','=','cp_to_rw'),('direction_usb','=','bidirectional')]
            else:
                rule_search_domain = rule_search_domain + ['|',('direction_usb','=','rw_to_cp'),('direction_usb','=','bidirectional')]
                 
            ids = self.pool.get('sync.client.rule').search(cr, uid, rule_search_domain, context=context)
            
            for rule_id in ids:
                updates_count += sum(update_pool.create_update(
                    cr, uid, rule_id, session, context=context))
                if logger:
                    logger.replace(logger_index, _('Update(s) created: %d' % updates_count))
                    
            if not updates_count and logger:
                logger.switch('status', 'ok')
            if logger:
                logger.write()
                
            return updates_count
        
        updates_count = create_update(session)
        return len(update_pool.search(cr, uid, [('sent','=',False)]))
    
    @sync_process(step='msg_push', need_connection=False, defaults_logger={'usb':True})
    def usb_create_message(self, cr, uid, context=None):
        context = context or {}
        message_pool = self.pool.get('sync_remote_warehouse.message_to_send')
        rule_pool = self.pool.get("sync.client.message_rule")

        messages_count = 0
        for rule in rule_pool.browse(cr, uid, rule_pool.search(cr, uid, [('usb','=',True)], context=context), context=context):
            messages_count += message_pool.create_from_rule(cr, uid, rule, context=context)
        
        if messages_count:
            cr.commit()
        
        # return number of messages to send
        return len(message_pool.search(cr, uid, [('sent','=',False)], context=context))
        
        
    def usb_push(self, cr, uid, context):
        """
        Create updates, create message, package into zip, attach to entity and increment entity usb sync step
        """
        context = context or {}
        context.update({'offline_synchronization' : True})
        updates_count = deletions_count = messages_count = 0
        
        session = str(uuid.uuid1())
        entity = self.get_entity(cr, uid, context=context)
        self.write(cr, uid, [entity.id], {'session_id' : session}) 
        
        # get update and message data
        updates = self.usb_create_update(cr, uid, session, context=context)
        messages = self.usb_create_message(cr, uid, context=context)
        
        # compress into zip
        if updates or messages:
            updates_count, deletions_count, messages_count = self.usb_create_zip(cr, uid, context=context)
        
        # cleanup
        self.usb_validate_push(cr, uid, context=context)
        self.write(cr, uid, entity.id, {'session_id' : ''}, context=context)
        
        # advance step if there was something to push
        if any((updates_count, deletions_count, messages_count)):
            self._usb_update_sync_step(cr, uid, 'push_performed')
        
        # return 
        return (updates_count, deletions_count, messages_count)
    
    @sync_process(step='data_push', need_connection=False, defaults_logger={'usb':True})
    def usb_validate_push(self, cr, uid, context=None):
        """
        Update update_to_send records with new usb_sync_date
        """
        # init
        entity = self.get_entity(cr, uid, context)
        
        # check step
        if entity.usb_sync_step not in ['pull_performed','first_sync']:
            raise osv.except_osv('Cannot Validated', 'We cannot Validate the Push until we have performed one')
        
        # get session id and latest updates
        session_id = entity.session_id
        update_pool = self.pool.get('sync_remote_warehouse.update_to_send')
        update_ids = update_pool.search(cr, uid, [('session_id', '=', session_id)], context=context)
        
        # mark latest updates as sync finished (set usb_sync_date) and clear entity session_id
        update_pool.sync_finished(cr, uid, update_ids, sync_field='usb_sync_date', context=context)
    
    @sync_process(step='data_pull', need_connection=False, defaults_logger={'usb':True})
    def usb_pull_update(self, cr, uid, uploaded_file_base64, context=None):
        """
        Takes the base64 for the uploaded zip file, unzips the csv files, parses them and inserts them into the database.
        @param uploaded_file_base64: The Base64 representation of a file - direct from the OpenERP api, i.e. wizard_object.pull_data
        @return: A dictionary of the CSV files enclosed in the ZIP, with their import status. Refer to STATIC error codes in this file
        """
        
        if self.get_entity(cr, uid, context).usb_sync_step not in ['push_performed', 'first_sync']:
            raise osv.except_osv('Cannot Pull', 'We cannot perform a Pull until we have performed a Pushed')
        
        entity = self.get_entity(cr, uid)
        context = context or {}
        logger = context.get('logger', None)
        if logger:
            logger_index = logger.append()
        
        # decode base64 and unzip
        try:
            uploaded_file = base64.decodestring(uploaded_file_base64)
            
            zip_stream = StringIO(uploaded_file)
            zip_file = ZipFile(zip_stream, 'r')
            
            update_received_model_name = 'sync_remote_warehouse.update_received'
            message_received_model_name = 'sync_remote_warehouse.update_received'
            
            update_rule_file = 'update_rules.txt'
            message_rule_file = 'message_rules.txt'
            
            files = [
                '%s.csv' % update_received_model_name,
                '%s.csv' % message_received_model_name,
                update_rule_file,
                message_rule_file,
            ]
            
            if not all([(file in zip_file.namelist()) for file in files]):
                raise osv.except_osv(_('Invalid USB Synchronisation Data'), _('The zip file you uploaded does not have all the data required for a pull. You must re-download the data from the other server.'))
                
            # if instance is remote warehouse, get update and message update_rules from zip file and import them 
            if entity.usb_instance_type == 'remote_warehouse': 
                update_rules = zip_file.read(update_rule_file)
                update_rules = eval(update_rules)
                self.pool.get('sync.client.rule').save(cr, uid, update_rules, context=context)
            
                if logger:
                    logger.append(_('Update Rules imported: %d' % len(update_rules)))
                
                message_rules = zip_file.read(message_rule_file)
                message_rules = eval(message_rules)
                self.pool.get('sync.client.message_rule').save(cr, uid, message_rules, context=context)
            
                if logger:
                    logger.append(_('Message Rules imported: %d' % len(message_rules)))
                
            # get CSV object to read data
            csv_file = zip_file.read('%s.csv' % update_received_model_name)
            csv_reader = csv.reader(StringIO(csv_file))
            
            import_data = {}
            results = {}
            first = True
            fields = []
            data = []
            
            # loop through csv rows and insert into fields or data array
            for row in csv_reader:
                if first:
                    fields = row
                    first = False
                else:
                    data.append(row)
                    
            zip_file.close()
            
            if logger:
                logger.replace(logger_index, _('Updates to import: %d' % len(data)))
            
            # insert into import_data
            import_data['fields'] = fields
            import_data['data'] = data
            
            # do importation and set result[model] = [True/False, [any error messages,...]]
            model_pool = self.pool.get(update_received_model_name)
            import_error = None
            
            try:
                model_pool.import_data(cr, uid, fields, data, context=context)
            except Exception as e:
                import_error =  '%s %s: %s' % (_('Import Error: '), type(e), str(e))
            except KeyError as e:
                import_error =  '%s %s: %s' % (_('Import Error: '), type(e), str(e)) 
            except ValueError as e:
                import_error =  '%s %s: %s' % (_('Import Error: '), type(e), str(e))
            
            # run updates
            updates_ran = None
            run_error = ''
            context.update({'update_received_model':'sync_remote_warehouse.update_received'})
            
            if not import_error:
                if logger:
                    logger.replace(logger_index, _('Updates imported: %d' % len(data)))
                try:
                    updates_ran = self.execute_updates(cr, uid, context=context)
                    self._usb_update_sync_step(cr, uid, 'pull_performed')
                    logger.switch('status','ok')
                except AttributeError, e:
                    run_error = '%s: %s' % (type(e), str(e))
                    logger.append(_('Error while executing updates: %s' % run_error))
            else:
                if logger:
                    logger.replace(logger_index, _('Error occured during import: %s' % import_error))
                    
            logger.switch('status', 'ok')
        except osv.except_osv, e:
            if logger:
                logger_index = logger_index or logger.append()
                logger.replace(logger_index, _(e.name + ': ' + e.value))
                logger.switch('status','failed')
            raise e
        except Exception, e:
            if logger:
                logger_index = logger_index or logger.append()
                logger.replace(logger_index, _('Error while reading uploaded zip file: %s' % str(e)))
                logger.switch('status', 'failed')
            raise e
        finally:
            if logger:
                logger.write()
    
        # increment usb sync step and return results
        return (len(data), import_error, updates_ran, run_error)
        
Entity()
