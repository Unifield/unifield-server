from osv import osv, fields, orm
import base64
from zipfile import ZipFile
from cStringIO import StringIO
import csv
import copy
from tools.translate import _
from sync_client import sync_process
import os
from datetime import datetime
import uuid

class Entity(osv.osv):
    _inherit = 'sync.client.entity'

    _columns = {
        # used to determine which sync rules to use 
        'usb_instance_type': fields.selection((('',''),('central_platform','Central Platform'),('remote_warehouse','Remote Warehouse')), string='USB Instance Type'),
        
        # used to ignore all data older than this date when syncing as it is already in the db
        'clone_date': fields.datetime('Backup Date And Time', help='The date that the Central Platform database was backed up to provide the seed data for the Remote Warehouse'),
        
        # the step of the synchronisation process - first_sync, pull_performed, push_performed
        'usb_sync_step': fields.selection((('first_sync','First Synchronisation'), ('pull_performed','Pull Performed'), ('push_performed', 'Push Performed')), 'USB Synchronisation Step'), 
        
        # used to make sure user does not try to import old data
        'usb_last_push_date': fields.datetime('Last Push Date', help='The date and time of the last Push'),
        'usb_last_push_file': fields.binary('Last Push File', help='The zip file that was generated by the last Push'),
    }
    
    _defaults = {
        'usb_sync_step': 'first_sync',
        'usb_instance_type': '',
    }
    
    IMPORT_ERROR_NOT_CSV = 'Not a CSV file'
    IMPORT_ERROR_NOT_OBJECT = 'Could not find object in object pool'
    
    def _update_usb_sync_step(self, cr, uid, step):
        entity = self.get_entity(cr, uid)
        return self.write(cr, uid, entity.id, {'usb_sync_step': step})
    
    def create_update_zip(self, cr, uid, context=None):
        """
        Create packages out of all total_updates marked as "to send", format as CSV, zip and attach to entity record 
        """
        
        csv_contents = []
        context = context or {}
        logger = context.get('logger', None)
        if logger:
            logger_index = logger.append()
        
        def create_package():
            return self.pool.get('sync_remote_warehouse.update_to_send').create_package(cr, uid, entity.session_id, max_packet_size)

        def add_and_mark_as_sent(ids, packet):
            """
            add the package contents to the csv_contents dictionary and mark the package as sent
            @return: (number of total_updates, number of delete_sdref rules)
            """
            
            # create header row if needed
            columns = ['source', 'model', 'version', 'fields', 'values', 'sdref', 'is_deleted']
            if not csv_contents:
                csv_contents.append(columns)
            
            # insert update data
            for update in packet['load']:
                csv_contents.append([
                    entity.name,
                    packet['model'], # model
                    update['version'],
                    packet['fields'],
                    update['values'],
                    update['sdref'],
                    False,
                ])
                
            # insert delete data
            for delete_sdref in packet['unload']:
                csv_contents.append([
                    entity.name,
                    packet['model'],
                    '',
                    '',
                    '',
                    delete_sdref,
                    True,
                ])
            
            # mark updates_to_send as sent and return number of records processed
            self.pool.get('sync_remote_warehouse.update_to_send').write(cr, uid, ids, {'sent' : True}, context=context)
            return (len(packet['load']), len(packet['unload']))
        
        _rules_serialization_mapping = {
            'id' : 'server_id',
            'name' : 'name',
            'owner_field' : 'owner_field',
            'model' : 'model',
            'domain' : 'domain',
            'sequence_number' : 'sequence_number',
            'included_fields' : 'included_fields',
            'can_delete' : 'can_delete',
            'usb' : 'usb',
            'active': 'active',
            'direction_usb' : 'direction_usb',
        }
    
        def _serialize_rule(rule_pool, cr, uid, ids, context=None):
            rules_data = []
            for rule in rule_pool.browse(cr, uid, ids, context=context):
                rules_data.append(dict(
                    (data, rule[column]) for column, data
                        in _rules_serialization_mapping.items()
                ))
            return rules_data

        # get number of update to process
        updates_todo = self.pool.get('sync_remote_warehouse.update_to_send').search(cr, uid, [('sent','=',False)], count=True, context=context)
        if updates_todo == 0:
            return 0, 0
        
        if logger:
            logger.replace(logger_index, _('Updates to package: %d' % updates_todo))
            logger.write()
        
        # prepare some variables and create the first package
        max_packet_size = self.pool.get("sync.client.sync_server_connection")._get_connection_manager(cr, uid, context=context).max_size
        entity = self.get_entity(cr, uid, context=context)

        total_updates, total_deletions = 0, 0
        package = create_package()
        
        while package:
            # add the package to the csv_contents dictionary and mark it has 'sent'
            new_updates, new_deletions = add_and_mark_as_sent(*package)
            
            # add new updates and deletions to total
            total_updates += new_updates
            total_deletions += new_deletions
            
            # update the log entry with the new total
            if logger:
                logger.replace(logger_index, _("Updates packaged: %d updates and %d deletions of %d total") % (total_updates, total_deletions, updates_todo))
                logger.write()
                
            # create next package
            package = create_package()

        # finished all packages so update logger
        if logger and (total_updates or total_deletions):
            logger.replace(logger_index, _("Update packaging complete: %d updates and %d deletions = total of %d") % (total_updates, total_deletions, (total_updates + total_deletions)))
            
        # create csv file
        try:
            csv_string_io = StringIO()
            csv_writer = csv.writer(csv_string_io, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            for data_row in csv_contents:
                csv_writer.writerow(data_row)
                    
            # create rules file to send to remote warehouse if instance is central platform
            if entity.usb_instance_type == 'central_platform': 
                rule_pool = self.pool.get('sync.client.rule')
                rule_ids = rule_pool.search(cr, uid, [('usb','=',True),'|',('direction_usb','=','rw_to_cp'),('direction_usb','=','bidirectional')])
                rules_serialized = _serialize_rule(rule_pool, cr, uid, rule_ids, context=context)
                
                rules_string_io = StringIO()
                rules_string_io.write(str(rules_serialized))
                    
            # compress csv file into zip
            zip_file_string_io = StringIO()
            zip_base64_output = StringIO()
            with ZipFile(zip_file_string_io, 'w') as zip_file:
                zip_file.writestr('sync_remote_warehouse.update_received.csv', csv_string_io.getvalue())
            
                if entity.usb_instance_type == 'central_platform': 
                    zip_file.writestr('rules.txt', rules_string_io.getvalue())
                    
                # add to entity object
            zip_file_contents = zip_file_string_io.getvalue()
            zip_base64 = base64.encodestring(zip_file_contents) 
            zip_base64_output.close()
    
            # clean up
            csv_string_io.close()
            rules_string_io.close()
            zip_file_string_io.close()
    
            # attach file to entity and delete 
            self.write(cr, uid, entity.id, {'usb_last_push_file': zip_base64, 'usb_last_push_date': datetime.now()})
            
            if logger:
                logger.switch('status', 'ok')
        except Exception, e:
            if logger:
                logger.append(_('Error while creating zip file from updates_to_send: %s' % str(e)))
                logger.switch('data_push', 'failed')
            raise
        finally:
            if logger:
                logger.write()
        
        return (total_updates, total_deletions)
    
    def create_usb_update(self, cr, uid, context=None):
        context = context or {}
        logger = context.get('logger', None)
        if logger:
            logger_index = logger.append()
        updates = self.pool.get('sync_remote_warehouse.update_to_send')
        
        def prepare_update(session):
            updates_count = 0
            
            rule_search_domain = [('usb','=',True)]
            if entity.usb_instance_type == 'central_platform':
                rule_search_domain = rule_search_domain + ['|',('direction_usb','=','cp_to_rw'),('direction_usb','=','bidirectional')]
            else:
                rule_search_domain = rule_search_domain + ['|',('direction_usb','=','rw_to_cp'),('direction_usb','=','bidirectional')]
                 
            ids = self.pool.get('sync.client.rule').search(cr, uid, rule_search_domain, context=context)
            
            for rule_id in ids:
                updates_count += sum(updates.create_update(
                    cr, uid, rule_id, session, context=context))
                if logger:
                    logger.replace(logger_index, _('Update(s) created: %d' % updates_count))
                    
            if not updates_count and logger:
                logger.switch('status', 'ok')
            if logger:
                logger.write()
                
            return updates_count
        
        entity = self.get_entity(cr, uid, context)
        session = str(uuid.uuid1())
            
        updates_count = prepare_update(session)
        if updates_count > 0:
            self.write(cr, uid, [entity.id], {'session_id' : session})
        return updates_count
    
    @sync_process(step='data_push', need_connection=False, defaults_logger={'usb':True})
    def usb_push_update(self, cr, uid, ids, context=None):
        
        context = context or {}
        context.update({
            'update_to_send_model': 'sync_remote_warehouse.update_to_send', 
            'last_sync_date_field': 'usb_sync_date'
        })
        entity = self.get_entity(cr, uid, context)
        
        if entity.usb_sync_step not in ['pull_performed', 'first_sync']:
            raise osv.except_osv('Cannot Push', 'We cannot perform a Push until we have Validated the last Pull')
        
        # update rules then create updates_to_send
        updates_count = self.create_usb_update(cr, uid, context=context)
        cr.commit() # commit because last thing done in create_update is set the session_id
        
        if not updates_count:
            return 0
        
        # add updates and rules to downloadable zip 
        cr.commit()
        updates, deletions = self.create_update_zip(cr, uid, context=context)
        
        # successful, so update records represented by updates_to_send with new usb_sync_date then clear session_id 
        self.usb_validate_push(cr, uid, ids, context=context)
        self.write(cr, uid, entity.id, {'session_id' : ''}, context=context)
        
        # increment usb sync step
        self._update_usb_sync_step(cr, uid, 'push_performed')
        
        return updates, deletions
    
    @sync_process(step='data_push', need_connection=False, defaults_logger={'usb':True})
    def usb_validate_push(self, cr, uid, ids, context=None):
        """
        Update update_to_send records with new usb_sync_date
        """
        # init
        entity = self.get_entity(cr, uid, context)
        
        # check step
        if entity.usb_sync_step not in ['pull_performed','first_sync']:
            raise osv.except_osv('Cannot Validated', 'We cannot Validate the Push until we have performed one')
        
        # get session id and latest updates
        session_id = entity.session_id
        update_pool = self.pool.get('sync_remote_warehouse.update_to_send')
        update_ids = update_pool.search(cr, uid, [('session_id', '=', session_id)], context=context)
        
        # mark latest updates as sync finished (set usb_sync_date) and clear entity session_id
        update_pool.sync_finished(cr, uid, update_ids, sync_field='usb_sync_date', context=context)
    
    @sync_process(step='data_pull', need_connection=False, defaults_logger={'usb':True})
    def usb_pull_update(self, cr, uid, uploaded_file_base64, context=None):
        """
        Takes the base64 for the uploaded zip file, unzips the csv files, parses them and inserts them into the database.
        @param uploaded_file_base64: The Base64 representation of a file - direct from the OpenERP api, i.e. wizard_object.pull_data
        @return: A dictionary of the CSV files enclosed in the ZIP, with their import status. Refer to STATIC error codes in this file
        """
        
        if self.get_entity(cr, uid, context).usb_sync_step not in ['push_performed', 'first_sync']:
            raise osv.except_osv('Cannot Pull', 'We cannot perform a Pull until we have performed a Pushed')
        
        entity = self.get_entity(cr, uid)
        context = context or {}
        logger = context.get('logger', None)
        if logger:
            logger_index = logger.append()
        
        # decode base64 and unzip
        try:
            uploaded_file = base64.decodestring(uploaded_file_base64)
            zip_stream = StringIO(uploaded_file)
            zip_file = ZipFile(zip_stream, 'r')
            update_received_model_name = 'sync_remote_warehouse.update_received'
        
            if '%s.csv' % update_received_model_name not in zip_file.namelist():
                raise osv.except_osv(_('USB Synchronisation Data Not Found'), _('The zip file must contain a file called sync_remote_warehouse.update_received.csv which contains the data for the USB Synchronisation. Please check your file...'))
                
            # get rules from zip file and import them if instance is remote warehouse
            if entity.usb_instance_type == 'remote_warehouse': 
                rules = zip_file.read('rules.txt')
                rules = eval(rules)
                self.pool.get('sync.client.rule').save(cr, uid, rules, context=context)
            
                if logger:
                    logger.append(_('Rules imported: %d' % len(rules)))
                
            # get CSV object to read data
            csv_file = zip_file.read('%s.csv' % update_received_model_name)
            csv_reader = csv.reader(StringIO(csv_file))
            
            import_data = {}
            results = {}
            first = True
            fields = []
            data = []
            
            # loop through csv rows and insert into fields or data array
            for row in csv_reader:
                if first:
                    fields = row
                    first = False
                else:
                    data.append(row)
                    
            zip_file.close()
            
            if logger:
                logger.replace(logger_index, _('Updates to import: %d' % len(data)))
            
            # insert into import_data
            import_data['fields'] = fields
            import_data['data'] = data
            
            # do importation and set result[model] = [True/False, [any error messages,...]]
            model_pool = self.pool.get(update_received_model_name)
            import_error = None
            
            try:
                model_pool.import_data(cr, uid, fields, data, context=context)
            except Exception as e:
                import_error =  '%s %s: %s' % (_('Import Error: '), type(e), str(e))
            except KeyError as e:
                import_error =  '%s %s: %s' % (_('Import Error: '), type(e), str(e)) 
            except ValueError as e:
                import_error =  '%s %s: %s' % (_('Import Error: '), type(e), str(e))
            
            # run updates
            updates_ran = None
            run_error = ''
            context.update({'update_received_model':'sync_remote_warehouse.update_received'})
            
            if not import_error:
                if logger:
                    logger.replace(logger_index, _('Updates imported: %d' % len(data)))
                try:
                    updates_ran = self.execute_updates(cr, uid, context=context)
                    self._update_usb_sync_step(cr, uid, 'pull_performed')
                    logger.switch('status','ok')
                except AttributeError, e:
                    run_error = '%s: %s' % (type(e), str(e))
                    logger.append(_('Error while executing updates: %s' % run_error))
            else:
                if logger:
                    logger.replace(logger_index, _('Error occured during import: %s' % import_error))
                    
            logger.switch('status', 'ok')
        except Exception, e:
            if logger:
                logger.append(logger_index, _('Error while reading uploaded zip file: %s' % str(e)))
                logger.switch('data_pull', 'failure')
            raise
        finally:
            if logger:
                logger.write()
    
        # increment usb sync step and return results
        return (len(data), import_error, updates_ran, run_error)
        
Entity()
